# -*- coding: utf-8 -*-
"""atividade9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_4sCuKqzxh7nMeqorOnnzZG61uiQWz2R
"""

import pandas as pd
import seaborn as sns
from matplotlib import pyplot
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import tree
from sklearn import svm
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import cross_val_score, KFold

# importando base de dados
a1_data = pd.read_csv("aula7/a1-in.csv", delimiter=',')
a1_data

a1_data.hist()
pyplot.show()

a1_data.shape

a1_data.dtypes

# verificando total de dados duplicados
duplicated = a1_data.duplicated().sum()

# verficando colunas com valores NaN
nan = a1_data.isna().any().sum()

print('valores duplicados: {}'.format(duplicated))
print('valores nulos: {}'.format(nan))

# one hot encoding
a1_encoded = pd.get_dummies(a1_data, columns=['SCHOOL', 'Class', 'CODER', 'Activity'])
a1_encoded

# transformando ontask em uma variável numérica
a1_encoded['ONTASK'].replace(['N','Y'], [0,1], inplace=True)
a1_encoded

'''
# Normalização
scaler = MinMaxScaler()
a1_scaled = scaler.fit_transform(a1_encoded)
a1_scaled = pd.DataFrame(a1_scaled, columns=a1_encoded.columns)
a1_scaled
'''

# Gerando um heatmap para enteder quais variáveis estão mais correlacionadas
sns.heatmap(a1_encoded.corr(), xticklabels=True, yticklabels=True, annot=False, vmin=-1, vmax=1, cmap='coolwarm')
plt.show()

"""Variáveis usadas no artigo:

Gender, GRADE, Transitions/Durations, Activity_Dancing, Activity_Individual, Activity_Smallgroup, Activity_Testing, Activity_Wholecarpet, Activity_Wholedesks, TRANSITIONS
"""

# y -> target (variavel dependete)
# x -> variaveis independentes
y = a1_encoded[["ONTASK"]]
x = a1_encoded[["Gender", "GRADE", "Transitions/Durations", "Activity_Dancing", "Activity_Individual", "Activity_Smallgroup", "Activity_Testing", "Activity_Wholecarpet", "Activity_Wholedesks", "TRANSITIONS"]]

# Gerando um novo heatmap apenas com as variáveis selecionadas para o treinamento
sns.heatmap(x.corr(), xticklabels=True, yticklabels=True, annot=False, vmin=-1, vmax=1, cmap='coolwarm')
plt.show()

# fazendo split do dataset
x_train, x_test, y_train, y_test = train_test_split(x, y)

"""# **Diferentes modelos**

1.   Árvores de decisão
2.   SVM
3.   Regressão probabilistica


"""

# criando um modelo de árvores de decisão
clf = tree.DecisionTreeClassifier()
clf = clf.fit(x_train, y_train)

k = 10
kf = KFold(n_splits=k, shuffle=True, random_state=42)

accuracy_scores = cross_val_score(clf, x_train, y_train, cv=kf, scoring='accuracy')

for i, accuracy in enumerate(accuracy_scores, 1):
  print(f'Fold {i}: Acurácia = {accuracy:.2f}')

mean_accuracy = np.mean(accuracy_scores)
print(f'Média de acurácia: {mean_accuracy:.2f}\n')

y_pred = clf.predict(x_test)

# matriz de confusão
cm = confusion_matrix(y_test, y_pred)

# acurácia
acc = accuracy_score(y_test, y_pred)

# precisão
precision = precision_score(y_test, y_pred, average=None, zero_division=1)

# recall
recall = recall_score(y_test, y_pred, average=None, zero_division=1)

# acurácia balanceada
ba = balanced_accuracy_score(y_test, y_pred)

# kappa
kappa = cohen_kappa_score(y_test, y_pred)

print(clf.score(x_train, y_train))

print(classification_report(y_test, y_pred, zero_division=1))

grade_predictions = pd.DataFrame(y_pred, columns=['ONTASK'])

# criando um modelo de svm
svm_model = svm.SVC(kernel='sigmoid')
svm_model.fit(x_train, y_train.values.ravel())

k = 10
kf = KFold(n_splits=k, shuffle=True, random_state=42)

accuracy_scores = cross_val_score(svm_model, x_train, y_train, cv=kf, scoring='accuracy')

for i, accuracy in enumerate(accuracy_scores, 1):
  print(f'Fold {i}: Acurácia = {accuracy:.2f}')

mean_accuracy = np.mean(accuracy_scores)
print(f'Média de acurácia: {mean_accuracy:.2f}\n')

y_pred = svm_model.predict(x_test)

# matriz de confusão
cm = confusion_matrix(y_test, y_pred)

# acurácia
acc = accuracy_score(y_test, y_pred)

# precisão
precision = precision_score(y_test, y_pred, average=None, zero_division=1)

# recall
recall = recall_score(y_test, y_pred, average=None, zero_division=1)

# f1 score
f1 = f1_score(y_test, y_pred, average=None)

# acurácia balanceada
ba = balanced_accuracy_score(y_test, y_pred)

# kappa
kappa = cohen_kappa_score(y_test, y_pred)

print(svm_model.score(x_train, y_train))

print(classification_report(y_test, y_pred, zero_division=1))

grade_predictions = pd.DataFrame(y_pred, columns=['ONTASK'])

# criando um modelo de regressão logistica
pr_model = LogisticRegression(solver='lbfgs', max_iter=5000)
pr_model.fit(x_train, y_train.values.ravel())

k = 10
kf = KFold(n_splits=k, shuffle=True, random_state=42)

accuracy_scores = cross_val_score(pr_model, x_train, y_train, cv=kf, scoring='accuracy')

for i, accuracy in enumerate(accuracy_scores, 1):
  print(f'Fold {i}: Acurácia = {accuracy:.2f}')

mean_accuracy = np.mean(accuracy_scores)
print(f'Média de acurácia: {mean_accuracy:.2f}\n')

y_pred = pr_model.predict(x_test)

# matriz de confusão
cm = confusion_matrix(y_test, y_pred)

# acurácia
acc = accuracy_score(y_test, y_pred)

# precisão
precision = precision_score(y_test, y_pred, average=None, zero_division=1)

# recall
recall = recall_score(y_test, y_pred, average=None, zero_division=1)

# f1 score
f1 = f1_score(y_test, y_pred, average=None)

# acurácia balanceada
ba = balanced_accuracy_score(y_test, y_pred)

# kappa
kappa = cohen_kappa_score(y_test, y_pred)

print(pr_model.score(x_train, y_train))

print(classification_report(y_test, y_pred, zero_division=1))

grade_predictions = pd.DataFrame(y_pred, columns=['ONTASK'])

"""# Os resultados de acurácia dos algoritmos foram:

**Utilizando 5 folds**

árvores: 68%

SMV: 56%

Regressão probabilistica: 67%

Utilizando estes três algoritmos de classificação, é possível notear que as árvores e a regressão tiveram um resultado semelhante com uma leve vantagem para as árvores. Já o algoritmo de SVM teve a pior acurácia.

**Utilizando 10 folds**

árvores: 68%

SMV: 56%

Regressão probabilistica: 67%

Os testes com 10 folds mostram resultados basicamente iguais na média final.
"""

