# -*- coding: utf-8 -*-
"""atividade9-afg.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FIL9Sf5CZT5VAs_2oe9D_K4d_PhDYZfh
"""

import pandas as pd
import seaborn as sns
from matplotlib import pyplot
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import tree
from sklearn import svm
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_regression
from sklearn.feature_selection import RFECV
from sklearn import datasets, linear_model

# importando base de dados
a1_data = pd.read_csv("aula7/a1-in.csv", delimiter=',')
a1_data

a1_data.hist()
pyplot.show()

a1_data.shape

a1_data.dtypes

# verificando total de dados duplicados
duplicated = a1_data.duplicated().sum()

# verficando colunas com valores NaN
nan = a1_data.isna().any().sum()

print('valores duplicados: {}'.format(duplicated))
print('valores nulos: {}'.format(nan))

# one hot encoding
a1_encoded = pd.get_dummies(a1_data, columns=['SCHOOL', 'Class', 'CODER', 'Activity'])
a1_encoded

# transformando ontask em uma variável numérica
a1_encoded['ONTASK'].replace(['N','Y'], [0,1], inplace=True)
a1_encoded

"""Foi criada uma variável que é "transitions time per student".
Essa variável basicamente multiplica o número de transições que o professor teve em uma aula pelo tempo de cada transição. Existe uma variável total time no dataset, mas ela parece levar em consideração algum outro tipo de parâmetro. Então ter o valor final das paralições que cada aluno gerou, me parece válido.
"""

# criando uma nova feature
a1_encoded['transitions time per student'] = 'NaN'
for index, row in a1_encoded.iterrows():
    r = row['TRANSITIONS'] * row['Transitions/Durations']
    a1_encoded.loc[index,'transitions time per student'] =  r
a1_encoded

a1_encoded_copy = a1_encoded

a1_encoded_copy.shape

# Gerando um heatmap para enteder quais variáveis estão mais correlacionadas
sns.heatmap(a1_encoded.corr(), xticklabels=True, yticklabels=True, annot=False, vmin=-1, vmax=1, cmap='coolwarm')
plt.show()

# eliminação de features com correlação entre si
corr_matrix = a1_encoded.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))

to_drop = [column for column in upper.columns if any(upper[column] > 0.6)]
print(to_drop)

#result = a1_encoded.drop(columns=to_drop)
#result.head(6)

# y -> target (variavel dependete)
# x -> variaveis independentes
y = a1_encoded[["ONTASK"]]
x = a1_encoded[["Gender", "GRADE", "Transitions/Durations", "Activity_Dancing", "Activity_Individual", "Activity_Smallgroup", "Activity_Testing", "Activity_Wholecarpet", "Activity_Wholedesks", "TRANSITIONS", "transitions time per student"]]

# Gerando um novo heatmap apenas com as variáveis selecionadas para o treinamento
sns.heatmap(x.corr(), xticklabels=True, yticklabels=True, annot=False, vmin=-1, vmax=1, cmap='coolwarm')
plt.show()

# fazendo split do dataset
x_train, x_test, y_train, y_test = train_test_split(x, y)

#validação cruzada dos modelos
salpes = 10000
features = 5
classes = 2
k = 5

kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)

models = {
	'Logistic Regression': LogisticRegression(),
	'SVM': svm.SVC(kernel='sigmoid'),
	'Decision Tree': DecisionTreeClassifier()
}

scores = {}

for model_name, model in models.items():
	accuracy_scores = cross_val_score(model, x_train, y_train.values.ravel(), cv = kf, scoring = 'accuracy')
	mean_accuracy = np.mean(accuracy_scores)
	scores[model_name] = mean_accuracy

# melhor modelo
best = max(scores, key=scores.get)
best = models[best]
best

# criando um modelo de árvores de decisão usando o GridSearch
parameters = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}
arv = tree.DecisionTreeClassifier()
clf = GridSearchCV(arv, parameters)
clf = clf.fit(x_train, y_train)
y_pred_test = clf.predict(x_test)
y_pred_train = clf.predict(x_train)

train_acc = accuracy_score(y_train, y_pred_train)
test_acc = accuracy_score(y_test, y_pred_test)
print(f'Acc dados de treino:  {train_acc}')
print(f'Acc dados de teste: {test_acc}')

# criando um modelo de árvores de decisão usando pipeline
pipe = Pipeline([('scaler', StandardScaler()), ('Decision Tree', DecisionTreeClassifier())])
print(f'Acc dados com pipeline: {pipe.fit(x_train, y_train).score(x_test, y_test)}')

"""Os resultados obtidos foram:

Acurácia com GridSearch: 0.6899221078949899 e 0.6758978797057551, enquanto a acurácia obtida com pipeline foi de 0.6761863551132266 e anteriormente foi de 0.68. Os resultados se mostraram muito parecidos, talvez pelo conjunto de variáveis não mudado muito. Neste exemplo apenas a variável nova criada foi considerad, mas ela parece não ter feito efeito em termos de eficiência do modelo.

"""

# matriz de confusão
cm = confusion_matrix(y_test, y_pred_test)
print(cm)

# acurácia
acc = accuracy_score(y_test, y_pred_test)
print(f"Acurácia: {acc:.4f}")

# precisão
precision = precision_score(y_test, y_pred_test)
print(f"Precisão: {precision:.4f}")

# recall
recall = recall_score(y_test, y_pred_test)
print(f"Recall: {recall:.4f}")

# acurácia balanceada
ba = balanced_accuracy_score(y_test, y_pred_test)
print(f"Acurácia balanceada: {ba:.4f}")

# kappa
kappa = cohen_kappa_score(y_test, y_pred_test)
print(f"Kappa: {kappa:.4f}")

print(classification_report(y_test, y_pred_test, zero_division=1))

# Aplicando o RFECV
ols = linear_model.LinearRegression()
rfecv = RFECV(estimator=ols, step=1, scoring="neg_mean_squared_error")
rfecv.fit(a1_encoded_copy, y)
rfecv.transform(a1_encoded_copy)

print("Melhores features: ", rfecv.n_features_)

selected_indices = np.where(rfecv.support_)[0]

print("Selected feature indices: ", selected_indices)

best_features = a1_encoded_copy.iloc[:, [3,6,7,8,12,14,15,16,22,23,24,25,26,27,28,29,30,32,36,37,38,40,41,42,44,45]]
best_features

# Gerando um heatmap para enteder quais variáveis estão mais correlacionadas
sns.heatmap(best_features.corr(), xticklabels=True, yticklabels=True, annot=False, vmin=-1, vmax=1, cmap='coolwarm')
plt.show()

# y -> target (variavel dependete)
# x -> variaveis independentes
y = best_features[["ONTASK"]]
x = best_features[["Gender", "TRANSITIONS", "NumACTIVITIES", "Transitions/Durations", "SCHOOL_A", "SCHOOL_B", "SCHOOL_C", "Class_T0U", "Class_T0V", "Class_T6Q", "Class_T6S", "Class_T6T", "Class_T6V", "Class_T7Q", "Class_T7T", "Class_T7V", "Class_T8R", "Class_T9Q", "Class_T9S", "Class_T9T", "Class_T9V", "CODER_Y", "CODER_Z", "Activity_Individual", "Activity_Smallgroup"]]

# fazendo split do dataset
x_train, x_test, y_train, y_test = train_test_split(x, y)

# criando um modelo de árvores de decisão usando o GridSearch
parameters = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}
arv = tree.DecisionTreeClassifier()
clf = GridSearchCV(arv, parameters)
clf = clf.fit(x_train, y_train)
y_pred_test = clf.predict(x_test)
y_pred_train = clf.predict(x_train)

train_acc = accuracy_score(y_train, y_pred_train)
test_acc = accuracy_score(y_test, y_pred_test)
print(f'Acc dados de treino:  {train_acc}')
print(f'Acc dados de teste: {test_acc}')

# criando um modelo de árvores de decisão usando pipeline
pipe = Pipeline([('scaler', StandardScaler()), ('Decision Tree', DecisionTreeClassifier())])
print(f'Acc dados com pipeline: {pipe.fit(x_train, y_train).score(x_test, y_test)}')

# matriz de confusão
cm = confusion_matrix(y_test, y_pred_test)
print(cm)

# acurácia
acc = accuracy_score(y_test, y_pred_test)
print(f"Acurácia: {acc:.4f}")

# precisão
precision = precision_score(y_test, y_pred_test)
print(f"Precisão: {precision:.4f}")

# recall
recall = recall_score(y_test, y_pred_test)
print(f"Recall: {recall:.4f}")

# acurácia balanceada
ba = balanced_accuracy_score(y_test, y_pred_test)
print(f"Acurácia balanceada: {ba:.4f}")

# kappa
kappa = cohen_kappa_score(y_test, y_pred_test)
print(f"Kappa: {kappa:.4f}")

"""Os resultados finais após rodar o RFECV foram um pouco melhores.


Acc dados de treino:  0.6800653909029715

Acc dados de teste: 0.6914755517092168

Acc dados com pipeline: 0.6921967402278956
"""